"""
src/core/data_processor.py
Module x·ª≠ l√Ω v√† l√†m s·∫°ch d·ªØ li·ªáu logs t·ª´ pfSense firewall
T·∫≠p trung v√†o behavior patterns, kh√¥ng ph·ª• thu·ªôc v√†o IP identifiers
"""

import json
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Union
from pathlib import Path
import sys
import pickle
from datetime import datetime

sys.path.append(str(Path(__file__).parent.parent.parent))
from src.utils.logger import get_module_logger


class DataProcessor:
    """
    Class x·ª≠ l√Ω d·ªØ li·ªáu logs t·ª´ firewall pfSense
    T·∫≠p trung v√†o behavioral features, lo·∫°i b·ªè IP-based features
    """
    
    def __init__(self, config: Dict):
        """
        Kh·ªüi t·∫°o DataProcessor
        
        Args:
            config: Dictionary ch·ª©a c·∫•u h√¨nh t·ª´ config.yaml
        """
        self.config = config
        self.logger = get_module_logger("DataProcessor")
        self.df = None
        self.stats = {}
        
        self.logger.info("="*80)
        self.logger.success("Kh·ªüi t·∫°o DataProcessor - Behavior-Focused Mode")
        self.logger.info("="*80)
    
    
    def load_logs(self, filepath: str) -> pd.DataFrame:
        """
        Load d·ªØ li·ªáu logs t·ª´ file JSON ho·∫∑c JSON Lines (NDJSON)
        
        Args:
            filepath: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file logs (.json ho·∫∑c .jsonl)
            
        Returns:
            DataFrame ch·ª©a d·ªØ li·ªáu logs
        """
        try:
            self.logger.info(f"üìÇ ƒêang load logs t·ª´: {filepath}")
            
            filepath = Path(filepath)
            if not filepath.exists():
                raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {filepath}")
            
            data = []
            with open(filepath, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, start=1):
                    line = line.strip()
                    if not line:  # B·ªè qua d√≤ng tr·ªëng
                        continue
                    try:
                        json_obj = json.loads(line)
                        data.append(json_obj)
                    except json.JSONDecodeError as e:
                        self.logger.error(f"‚ùå L·ªói parse JSON ·ªü d√≤ng {line_num}: {e}")
                        self.logger.error(f"   N·ªôi dung d√≤ng (200 k√Ω t·ª± ƒë·∫ßu): {line[:200]}...")
                        raise
            
            if not data:
                raise ValueError("File log r·ªóng ho·∫∑c kh√¥ng ch·ª©a d·ªØ li·ªáu JSON h·ª£p l·ªá")
            
            # T·∫°o DataFrame
            self.df = pd.DataFrame(data)
            
            # H·ªó tr·ª£ c≈©: n·∫øu l√† JSON chu·∫©n (m·ªôt list l·ªõn), v·∫´n ho·∫°t ƒë·ªông
            # Nh∆∞ng ∆∞u ti√™n JSON Lines nh∆∞ tr√™n
            
            self.logger.success(f"‚úÖ Load th√†nh c√¥ng {len(self.df):,} b·∫£n ghi logs")
            self.stats['total_records'] = len(self.df)
            
            # Hi·ªÉn th·ªã sample
            self._display_sample()
            
            return self.df
            
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói khi load logs: {str(e)}")
            raise
    
    # Flatten Elasticsearch hit n·∫øu c·∫ßn
    def _flatten_elasticsearch_hit(self, hit: Dict) -> Dict:
        """
        Flatten Elasticsearch hit document
        
        Args:
            hit: Document t·ª´ Elasticsearch
            
        Returns:
            Flat dictionary
        """
        record = {}
        
        # L·∫•y _source ho·∫∑c fields
        if '_source' in hit:
            source = hit['_source']
        elif 'fields' in hit:
            source = {k: v[0] if isinstance(v, list) and len(v) == 1 else v 
                     for k, v in hit['fields'].items()}
        else:
            source = hit
        
        # Flatten nested fields
        for key, value in source.items():
            if isinstance(value, list) and len(value) == 1:
                record[key] = value[0]
            elif isinstance(value, list) and len(value) == 0:
                record[key] = None
            else:
                record[key] = value
        
        return record
    
    def _display_sample(self):
        """Hi·ªÉn th·ªã sample data"""
        if self.df is not None and len(self.df) > 0:
            self.logger.info("\nüìä Sample data (5 b·∫£n ghi ƒë·∫ßu):")
            sample_cols = ['@timestamp', 'action', 'src_ip', 'dst_ip', 'src_port', 
                          'dst_port', 'proto_name', 'length']
            available_cols = [c for c in sample_cols if c in self.df.columns]
            
            if available_cols:
                print(self.df[available_cols].head().to_string(index=False))
    
    def validate_data(self) -> bool:
        """
        Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa d·ªØ li·ªáu
        """
        try:
            self.logger.info("\nüîç B·∫Øt ƒë·∫ßu validate d·ªØ li·ªáu...")
            
            if self.df is None or len(self.df) == 0:
                self.logger.error("‚ùå DataFrame r·ªóng")
                return False
            
            # C√°c tr∆∞·ªùng b·∫Øt bu·ªôc cho behavior analysis
            required_fields = {
                'behavioral': ['src_port', 'dst_port', 'proto_name', 'action', 'dir'],
                'temporal': ['@timestamp'],
                'packet': ['length', 'ttl'],
                'context': ['src_ip', 'dst_ip']  # Ch·ªâ d√πng ƒë·ªÉ th·ªëng k√™, kh√¥ng train
            }
            
            missing_critical = []
            for category, fields in required_fields.items():
                for field in fields:
                    if field not in self.df.columns:
                        missing_critical.append(f"{field} ({category})")
            
            if missing_critical:
                self.logger.warning(f"‚ö†Ô∏è  Thi·∫øu tr∆∞·ªùng: {', '.join(missing_critical)}")
                # T·∫°o tr∆∞·ªùng thi·∫øu v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh
                for field in missing_critical:
                    field_name = field.split(' ')[0]
                    self.df[field_name] = None
            
            # Th·ªëng k√™ null values
            null_counts = self.df.isnull().sum()
            if null_counts.sum() > 0:
                self.logger.warning("\n‚ö†Ô∏è  Null values detected:")
                print(null_counts[null_counts > 0].to_string())
            
            self.logger.success("‚úÖ Validate ho√†n t·∫•t")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói validate: {str(e)}")
            return False


    def _normalize_list_columns(self):
        """
        Normalize c√°c c·ªôt ch·ª©a list v·ªÅ scalar ho·∫∑c string.
        X·ª≠ l√Ω ƒë·∫∑c th√π Elasticsearch array fields.
        
        Logic:
        - [] ‚Üí None
        - [single_value] ‚Üí single_value  
        - [val1, val2, ...] ‚Üí "val1,val2,..." (join string)
        - scalar ‚Üí scalar (kh√¥ng ƒë·ªïi)
        """
        list_columns = []
        
        # Ph√°t hi·ªán c√°c c·ªôt ch·ª©a list
        for col in self.df.columns:
            # Check n·∫øu b·∫•t k·ª≥ gi√° tr·ªã n√†o trong c·ªôt l√† list
            if self.df[col].apply(lambda x: isinstance(x, list)).any():
                list_columns.append(col)
        
        if not list_columns:
            self.logger.info("  ‚Ü≥ Kh√¥ng c√≥ c·ªôt list n√†o c·∫ßn normalize")
            return
        
        self.logger.info(f"  ‚Ü≥ Ph√°t hi·ªán {len(list_columns)} c·ªôt ch·ª©a list:")
        self.logger.info(f"    {', '.join(list_columns)}")
        
        for col in list_columns:
            def extract_value(x):
                """Tr√≠ch xu·∫•t gi√° tr·ªã t·ª´ list ho·∫∑c gi·ªØ nguy√™n scalar"""
                if isinstance(x, list):
                    if len(x) == 0:
                        return None  # Empty list ‚Üí None
                    elif len(x) == 1:
                        return x[0]  # Single value ‚Üí extract
                    else:
                        # Multiple values ‚Üí join th√†nh string
                        # D√πng str() ƒë·ªÉ ƒë·∫£m b·∫£o t·∫•t c·∫£ elements ƒë·ªÅu convert ƒë∆∞·ª£c
                        return ','.join(str(item) for item in x)
                return x  # Gi·ªØ nguy√™n scalar
            
            self.df[col] = self.df[col].apply(extract_value)
            self.logger.debug(f"    ‚Ä¢ Normalized: {col}")
        
        self.logger.success(f"  ‚Ü≥ ‚úÖ ƒê√£ normalize {len(list_columns)} c·ªôt")
    
    def clean_data(self) -> pd.DataFrame:
        """
        L√†m s·∫°ch d·ªØ li·ªáu: normalize lists, x·ª≠ l√Ω missing, duplicates, outliers
        
        Returns:
            pd.DataFrame: D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
        """
        try:
            self.logger.info("\nüßπ B·∫Øt ƒë·∫ßu l√†m s·∫°ch d·ªØ li·ªáu...")
            initial_count = len(self.df)
            
            # ‚úÖ B∆Ø·ªöC 1: Normalize list columns TR∆Ø·ªöC (critical!)
            self._normalize_list_columns()
            
            # ‚úÖ B∆Ø·ªöC 2: X√≥a duplicates (gi·ªù ƒë√£ an to√†n)
            dup_count = self.df.duplicated().sum()
            if dup_count > 0:
                self.df = self.df.drop_duplicates()
                self.logger.info(f"  ‚Ü≥ ƒê√£ x√≥a {dup_count:,} b·∫£n ghi tr√πng l·∫∑p")
            else:
                self.logger.info(f"  ‚Ü≥ Kh√¥ng c√≥ b·∫£n ghi tr√πng l·∫∑p")
            
            # ‚úÖ B∆Ø·ªöC 3: Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu
            self._convert_data_types()
            
            # ‚úÖ B∆Ø·ªöC 4: X·ª≠ l√Ω missing values
            self._handle_missing_values()
            
            # ‚úÖ B∆Ø·ªöC 5: X·ª≠ l√Ω outliers
            self._handle_outliers()
            
            # ‚úÖ B∆Ø·ªöC 6: Chu·∫©n h√≥a gi√° tr·ªã
            self._normalize_values()
            
            final_count = len(self.df)
            removed = initial_count - final_count
            
            self.stats['records_removed'] = removed
            self.stats['final_records'] = final_count
            
            self.logger.success(f"‚úÖ L√†m s·∫°ch ho√†n t·∫•t: {initial_count:,} ‚Üí {final_count:,} "
                            f"({removed:,} b·ªã lo·∫°i)")
            
            return self.df
            
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói l√†m s·∫°ch: {str(e)}")
            import traceback
            self.logger.error(traceback.format_exc())
            raise
    
    def _convert_data_types(self):
        """Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu"""
        self.logger.info("  ‚Ü≥ Chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu...")
        
        # Numeric fields
        numeric_fields = ['src_port', 'dst_port', 'length', 'ttl', 'id', 
                         'data_length', 'offset', 'tracker']
        for col in numeric_fields:
            if col in self.df.columns:
                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')
        
        # Timestamp
        if '@timestamp' in self.df.columns:
            self.df['@timestamp'] = pd.to_datetime(self.df['@timestamp'], errors='coerce')
        
        # String fields lowercase
        string_fields = ['action', 'proto_name', 'dir', 'tcp_flags']
        for col in string_fields:
            if col in self.df.columns:
                self.df[col] = self.df[col].astype(str).str.lower()
    
    def _handle_missing_values(self):
        """X·ª≠ l√Ω missing values"""
        self.logger.info("  ‚Ü≥ X·ª≠ l√Ω missing values...")
        
        before = self.df.isnull().sum().sum()
        
        # Critical fields: x√≥a d√≤ng thi·∫øu
        critical = ['action', 'proto_name', '@timestamp']
        self.df = self.df.dropna(subset=critical)
        
        # Ports: fill 0
        for col in ['src_port', 'dst_port']:
            if col in self.df.columns:
                self.df[col].fillna(0, inplace=True)
        
        # Direction: fill 'unknown'
        if 'dir' in self.df.columns:
            self.df['dir'].fillna('unknown', inplace=True)
        
        # Numeric: fill median
        numeric_cols = ['length', 'ttl', 'data_length']
        for col in numeric_cols:
            if col in self.df.columns and self.df[col].dtype in ['int64', 'float64']:
                median = self.df[col].median()
                self.df[col].fillna(median, inplace=True)
        
        after = self.df.isnull().sum().sum()
        self.logger.info(f"    ‚Ä¢ Missing: {before:,} ‚Üí {after:,}")
    
    def _handle_outliers(self):
        """X·ª≠ l√Ω outliers b·∫±ng IQR clipping"""
        self.logger.info("  ‚Ü≥ X·ª≠ l√Ω outliers (IQR method)...")
        
        numeric_cols = ['src_port', 'dst_port', 'length', 'ttl']
        total_outliers = 0
        
        for col in numeric_cols:
            if col not in self.df.columns:
                continue
            
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1
            
            lower = Q1 - 3 * IQR
            upper = Q3 + 3 * IQR
            
            outliers = ((self.df[col] < lower) | (self.df[col] > upper)).sum()
            total_outliers += outliers
            
            # Clip instead of remove
            self.df[col] = self.df[col].clip(lower=lower, upper=upper)
        
        if total_outliers > 0:
            self.logger.info(f"    ‚Ä¢ ƒê√£ clip {total_outliers:,} outliers")
    
    def _normalize_values(self):
        """Chu·∫©n h√≥a gi√° tr·ªã"""
        self.logger.info("  ‚Ü≥ Chu·∫©n h√≥a gi√° tr·ªã...")
        
        # Lowercase c√°c tr∆∞·ªùng text
        text_cols = ['action', 'proto_name', 'dir', 'reason']
        for col in text_cols:
            if col in self.df.columns:
                self.df[col] = self.df[col].str.strip().str.lower()
        
        # Strip whitespace t·ª´ IPs
        for col in ['src_ip', 'dst_ip']:
            if col in self.df.columns:
                self.df[col] = self.df[col].str.strip()
    
    def save_processed_data(self, output_path: str):
        """
        L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
        
        Args:
            output_path: ƒê∆∞·ªùng d·∫´n file output (.pkl ho·∫∑c .csv)
        """
        try:
            self.logger.info(f"\nüíæ L∆∞u d·ªØ li·ªáu v√†o: {output_path}")
            
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            
            if output_path.endswith('.pkl'):
                self.df.to_pickle(output_path)
            elif output_path.endswith('.csv'):
                self.df.to_csv(output_path, index=False)
            else:
                raise ValueError("Ch·ªâ h·ªó tr·ª£ .pkl ho·∫∑c .csv")
            
            self.logger.success(f"‚úÖ ƒê√£ l∆∞u {len(self.df):,} b·∫£n ghi")
            
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói l∆∞u file: {str(e)}")
            raise
    
    def get_statistics(self) -> Dict:
        """L·∫•y th·ªëng k√™ d·ªØ li·ªáu"""
        if self.df is None:
            return {}
        
        stats = {
            'total_records': len(self.df),
            'columns': list(self.df.columns),
            'action_dist': self.df['action'].value_counts().to_dict() if 'action' in self.df.columns else {},
            'protocol_dist': self.df['proto_name'].value_counts().to_dict() if 'proto_name' in self.df.columns else {},
            'missing_values': self.df.isnull().sum().to_dict()
        }
        
        return stats
    
    def print_summary(self):
        """In t√≥m t·∫Øt d·ªØ li·ªáu ƒë·∫πp"""
        self.logger.info("\n" + "="*80)
        self.logger.info("üìä T√ìM T·∫ÆT D·ªÆ LI·ªÜU")
        self.logger.info("="*80)
        
        if self.df is None:
            self.logger.warning("Ch∆∞a c√≥ d·ªØ li·ªáu")
            return
        
        print(f"\n{'T·ªïng s·ªë b·∫£n ghi:':<30} {len(self.df):>15,}")
        print(f"{'S·ªë c·ªôt:':<30} {len(self.df.columns):>15,}")
        
        if 'action' in self.df.columns:
            print(f"\n{'Action Distribution:':<30}")
            for action, count in self.df['action'].value_counts().items():
                pct = count / len(self.df) * 100
                print(f"  {action:<28} {count:>10,}  ({pct:>5.1f}%)")
        
        if 'proto_name' in self.df.columns:
            print(f"\n{'Protocol Distribution:':<30}")
            for proto, count in self.df['proto_name'].value_counts().head(5).items():
                pct = count / len(self.df) * 100
                print(f"  {proto:<28} {count:>10,}  ({pct:>5.1f}%)")
        
    print("\n" + "="*80 + "\n")


if __name__ == '__main__':
    import yaml
    
    with open('config/config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    processor = DataProcessor(config)
    df = processor.load_logs(config['data']['training_logs'])
    processor.validate_data()
    df_cleaned = processor.clean_data()
    processor.save_processed_data(config['data']['cleaned_logs'])
    processor.print_summary()
